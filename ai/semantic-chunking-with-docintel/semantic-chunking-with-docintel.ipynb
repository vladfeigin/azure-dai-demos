{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-retrieval-augmented-generation?view=doc-intel-4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (0.2.16)\n",
      "Requirement already satisfied: langchain-community in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (0.2.16)\n",
      "Requirement already satisfied: langchain-openai in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (0.1.23)\n",
      "Requirement already satisfied: langchainhub in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (0.1.21)\n",
      "Requirement already satisfied: openai in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (1.44.0)\n",
      "Requirement already satisfied: tiktoken in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: azure-ai-documentintelligence in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (1.0.0b4)\n",
      "Requirement already satisfied: azure-identity in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (1.17.1)\n",
      "Requirement already satisfied: azure-search-documents==11.6.0b3 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (11.6.0b3)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from azure-search-documents==11.6.0b3) (1.30.2)\n",
      "Requirement already satisfied: azure-common>=1.1 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from azure-search-documents==11.6.0b3) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from azure-search-documents==11.6.0b3) (0.6.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (3.10.5)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (0.2.38)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (0.2.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (0.1.116)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (2.9.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchainhub) (2.32.0.20240907)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from azure-identity) (43.0.1)\n",
      "Requirement already satisfied: msal>=1.24.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from azure-identity) (1.31.0)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from azure-identity) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from azure-core>=1.28.0->azure-search-documents==11.6.0b3) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from cryptography>=2.5->azure-identity) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity) (2.9.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from msal-extensions>=0.3.0->azure-identity) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.2 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.23.2)\n",
      "Requirement already satisfied: tzdata in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: pycparser in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.22)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/vladfeigin/myprojects/dai-demos/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv langchain langchain-community langchain-openai langchainhub openai tiktoken azure-ai-documentintelligence azure-identity azure-search-documents==11.6.0b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The environment variables are loaded from the `.env` file in the same directory as this notebook.\n",
    "\"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "_=load_dotenv()\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the file exists\n",
    "if not os.path.exists(\"./data/hyde.pdf\"):\n",
    "    raise Exception(\"The file ./data/hyde.pdf does not exist.\")\n",
    "\n",
    "pdf_path=\"./data/hyde.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits: 23\n"
     ]
    }
   ],
   "source": [
    "# Initiate Azure AI Document Intelligence to load the document. You can either specify file_path or url_path to load the document.\n",
    "loader = AzureAIDocumentIntelligenceLoader(file_path=pdf_path, api_key = doc_intelligence_key, api_endpoint = doc_intelligence_endpoint, api_model=\"prebuilt-layout\", api_version=\"2024-02-29-preview\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the document into chunks base on markdown headers.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    #('####', 'Header 4'),\n",
    "    #('#####'    , 'Header 5'),\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_string = docs[0].page_content\n",
    "splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(splits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersatd the Azure Docuement Intelligence chunking\n",
    "### TODO - Provide explanation here and why it's good for RAG "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the splitted documents and insert into Azure Search vector store\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "\n",
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"), \n",
    "    openai_api_version=\"2024-02-01\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5q47Xdc8WOXBcW2G1ittzKz691WeO1GdtBHCmTrk9AAzSeDQ18W6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['MjBjNGY4NWQtMmE1NS00Y2VlLTg4NDUtMDg5YWExOWYwOTg3',\n",
       " 'MmVkZjhiMDMtOThhYy00ZDg2LWE0NGQtZTM0OTFmODM3MzRk',\n",
       " 'YjgxZjczMzktOGFjYy00YWI1LWExZDYtZDZmZTc5YmM5MzU5',\n",
       " 'YjIxNzRhYjUtYTYxZS00ODZiLTgyOWEtODAwNDEwZmQyYzQ2',\n",
       " 'Mjg0ZGZjYWItNDFhMi00YzFiLWEzZDUtYTQwNGEyYjY2YjI2',\n",
       " 'MWI0NmNkZjctNmVkNy00YWZkLTg1NTktMTdlZjQzMDg4MDcy',\n",
       " 'YTFkYzQ5OTctMWI5MS00NTk0LWIzNjgtMGRiYzNlY2FiMTcz',\n",
       " 'MmUzZGFiY2EtM2M0NC00ZWUxLWI1ZDYtNTQ4ZTFkMTIwNzIw',\n",
       " 'ZTc2NWU0NTUtZDliMC00ZjdlLWExMjgtNTVjNjBlYzVmMmVj',\n",
       " 'ZGU3YmRiYTAtNzYyMS00YmJiLTk1YWQtNjlhN2VmMDRmNGJi',\n",
       " 'Yjg0MzYyMDUtNzdiMS00YTFmLWJiYWYtNTdhNzY5MTVhNjYx',\n",
       " 'NTZiZjk0MTMtYTIzNi00OGMxLWFmZmUtMDFhYjBiYTFmNTA5',\n",
       " 'NGUxNzQ1MjItZTI4Mi00ODc5LTgwZGMtMDFmMGRkYzYyYjMy',\n",
       " 'MDc1NjUyY2QtNDk1NS00Y2RkLWE1MGItNTgwMWMwYmM5YWEw',\n",
       " 'NTk5ZTdiYjItMmEwZS00ZjBhLWExOWMtNTFhMjE3OTVhMzRh',\n",
       " 'NzQ0NmVmMjktZThlNS00ZmViLWE2NDEtNjkwMGI1Mjc1Yjgz',\n",
       " 'N2U0Y2I0M2YtZjk2Yi00MDBlLTliNmQtNWUxZmU1NWVhM2Y2',\n",
       " 'NTk0NzE4MDctMjM1Yy00NmRiLWIyNjktODc4NWFhODYyNjc0',\n",
       " 'YTAwM2E5MzUtOTgxYi00NDAyLWEzY2QtYzY2ZmRhYjU4M2Uz',\n",
       " 'M2M1OTk2YzctMGZhMC00YmY1LWExNTEtODdhMjRkYjBiNjg5',\n",
       " 'YmY4MTMzNDMtODY3ZC00ODI3LWEwZGEtM2Y3OTJlZTA5ODY4',\n",
       " 'YWIxMDZlZmItN2RlMi00ZDY4LTliZWMtYmYwZGVkN2Y5MGY0',\n",
       " 'MGI5NGJlYjItYjU3MC00OWFiLTkyMjUtZDM3NzczMTZhZjE4']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store_address: str = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "vector_store_password: str = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "print(vector_store_password)\n",
    "\n",
    "index_name: str = \"langchain-aisearch-docintel-demo-index-1\"\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=aoai_embeddings.embed_query,\n",
    ")\n",
    "\n",
    "vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(metadata={'id': 'YzA0YjY1YjctOTI2Ny00YTdhLWIxNTYtY2EwYzc1YzMwYWFm', 'Header 1': '1 Introduction'}, page_content='Dense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown successful across tasks like web search, question answering, and fact verification. A variety of meth- ods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021) and task-specific  \\npre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) have been proposed to improve the effectiveness of supervised dense retrieval models.  \\nOn the other hand, zero-shot dense retrieval still remains difficult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios.  \\nIn this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). Ouyang et al. (2022) show that with a small amount of data, GPT-3 (Brown et al., 2020) models can be aligned  \\n<!-- Footnote=\"Equal contribution.\" -->  \\n<!-- Footnote=\"\"No models were trained or fine-tuned in making this pre- print. Our open source code is available at https://github. com/texttron/hyde.\" -->\\n:selected: :selected: :selected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :selected: :selected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :selected: :selected:<figure>  \\n![](figures/0)  \\n<!-- FigureContent=\"write a passage to answer the question how long does it take to remove HyDE How wisdom teeth are removed ... Some ... a few minutes, whereas wisdom tooth It usually takes between 30 others can take 20 minutes or minutes and two hours to remove a wisdom tooth ... longer .... write a scientific paper passage to answer the question How has the COVID-19 pandemic impacted ... depression and anxiety had >increased by 20% since the Contriever ... two studies investigating mental health? > COVID-19 patients ... significantly GPT start of the pandemic ... higher level of depressive ... 인간이 불을 사용한 기록은 약 write a passage in Korean to answer the 800만년 전부터 나타난다 ... question in detail ... 불을 처음 사용한 시기는 호모 에렉투스가 살았던 142만 년 전으 로 거슬러간다 ... 인간은 언제 불을 사용했는가? :unselected: :unselected: instruction :unselected: query generated document :unselected: real document\" -->  \\n<figcaption>  \\nFigure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries\\nwithout changing the underlying GPT-3 and Contriever/mContriever models.  \\n</figcaption>  \\n</figure>  \\nto human intent to follow instructions.  \\nWith these ingredients, we propose to pivot through Hypothetical Document Embeddings (HyDE), and decompose dense retrieval into two tasks, a generative task per- formed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder\\'s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks.  \\nHyDE appears unsupervised. No model is trained in HyDE: both the generative model and the con- trastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM.  \\nIn our experiments, we show HyDE using Instruct- GPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models significantly out- performs the previous state-of-the-art Contriever- only zero-shot no-relevance system on 11 queries  \\nsets, covering tasks like Web Search, Question Answering, Fact Verification and languages like Swahili, Korean, Japanese.'),\n",
      "  0.85015905),\n",
      " (Document(metadata={'id': 'YWI1OWU4MTgtMGNhYi00ZTI5LWIzZWUtY2M4NmViNjM2ZWE1', 'Header 1': '1 Introduction'}, page_content='Dense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown successful across tasks like web search, question answering, and fact verification. A variety of meth- ods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021) and task-specific  \\npre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) have been proposed to improve the effectiveness of supervised dense retrieval models.  \\nOn the other hand, zero-shot dense retrieval still remains difficult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios.  \\nIn this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). Ouyang et al. (2022) show that with a small amount of data, GPT-3 (Brown et al., 2020) models can be aligned  \\n<!-- Footnote=\"Equal contribution.\" -->  \\n<!-- Footnote=\"\"No models were trained or fine-tuned in making this pre- print. Our open source code is available at https://github. com/texttron/hyde.\" -->\\n:selected: :selected: :selected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :selected: :selected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :selected: :selected:<figure>  \\n![](figures/0)  \\n<!-- FigureContent=\"write a passage to answer the question how long does it take to remove HyDE How wisdom teeth are removed ... Some ... a few minutes, whereas wisdom tooth It usually takes between 30 others can take 20 minutes or minutes and two hours to remove a wisdom tooth ... longer .... write a scientific paper passage to answer the question How has the COVID-19 pandemic impacted ... depression and anxiety had >increased by 20% since the Contriever ... two studies investigating mental health? > COVID-19 patients ... significantly GPT start of the pandemic ... higher level of depressive ... 인간이 불을 사용한 기록은 약 write a passage in Korean to answer the 800만년 전부터 나타난다 ... question in detail ... 불을 처음 사용한 시기는 호모 에렉투스가 살았던 142만 년 전으 로 거슬러간다 ... 인간은 언제 불을 사용했는가? :unselected: :unselected: instruction :unselected: query generated document :unselected: real document\" -->  \\n<figcaption>  \\nFigure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries\\nwithout changing the underlying GPT-3 and Contriever/mContriever models.  \\n</figcaption>  \\n</figure>  \\nto human intent to follow instructions.  \\nWith these ingredients, we propose to pivot through Hypothetical Document Embeddings (HyDE), and decompose dense retrieval into two tasks, a generative task per- formed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder\\'s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks.  \\nHyDE appears unsupervised. No model is trained in HyDE: both the generative model and the con- trastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM.  \\nIn our experiments, we show HyDE using Instruct- GPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models significantly out- performs the previous state-of-the-art Contriever- only zero-shot no-relevance system on 11 queries  \\nsets, covering tasks like Web Search, Question Answering, Fact Verification and languages like Swahili, Korean, Japanese.'),\n",
      "  0.85015905),\n",
      " (Document(metadata={'id': 'YjgxZjczMzktOGFjYy00YWI1LWExZDYtZDZmZTc5YmM5MzU5', 'Header 1': '1 Introduction'}, page_content='Dense retrieval (Lee et al., 2019; Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown successful across tasks like web search, question answering, and fact verification. A variety of meth- ods such as negative mining (Xiong et al., 2021; Qu et al., 2021), distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021) and task-specific  \\npre-training (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022) have been proposed to improve the effectiveness of supervised dense retrieval models.  \\nOn the other hand, zero-shot dense retrieval still remains difficult. Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from new tasks. The MS- MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged query- document pairs, is arguably the most commonly used. As argued by Izacard et al. (2021), in prac- tice, however, the existence of such a large dataset cannot always be assumed. Even MS-MARCO re- stricts commercial use and cannot be adopted in a variety of real-world search scenarios.  \\nIn this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and gener- alize across tasks. As supervision is not available, we start by examining self-supervised representa- tion learning methods. Modern deep learning en- ables two distinct learning algorithms. At the token level, generative large language models (LLM) pre- trained on large corpus have demonstrated strong natural language understanding (NLU) and gen- eration (NLG) capabilities (Brown et al., 2020; Chen et al., 2021; Rae et al., 2021; Hoffmann et al., 2022; Thoppilan et al., 2022; Chowdhery et al., 2022). At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021; Gao and Callan, 2022). On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse un- seen instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). Ouyang et al. (2022) show that with a small amount of data, GPT-3 (Brown et al., 2020) models can be aligned  \\n<!-- Footnote=\"Equal contribution.\" -->  \\n<!-- Footnote=\"\"No models were trained or fine-tuned in making this pre- print. Our open source code is available at https://github. com/texttron/hyde.\" -->\\n:selected: :selected: :selected: :unselected: :selected: :unselected: :unselected: :unselected: :unselected: :selected: :selected: :unselected: :selected: :unselected: :unselected: :selected: :selected: :selected: :selected: :selected:<figure>  \\n![](figures/0)  \\n<!-- FigureContent=\"write a passage to answer the question how long does it take to remove HyDE How wisdom teeth are removed ... Some ... a few minutes, whereas wisdom tooth It usually takes between 30 others can take 20 minutes or minutes and two hours to remove a wisdom tooth ... longer .... write a scientific paper passage to answer the question How has the COVID-19 pandemic impacted ... depression and anxiety had >increased by 20% since the Contriever ... two studies investigating mental health? > COVID-19 patients ... significantly GPT start of the pandemic ... higher level of depressive ... 인간이 불을 사용한 기록은 약 write a passage in Korean to answer the 800만년 전부터 나타난다 ... question in detail ... 불을 처음 사용한 시기는 호모 에렉투스가 살았던 142만 년 전으 로 거슬러간다 ... 인간은 언제 불을 사용했는가? :unselected: :unselected: instruction :unselected: query generated document :unselected: real document\" -->  \\n<figcaption>  \\nFigure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries\\nwithout changing the underlying GPT-3 and Contriever/mContriever models.  \\n</figcaption>  \\n</figure>  \\nto human intent to follow instructions.  \\nWith these ingredients, we propose to pivot through Hypothetical Document Embeddings (HyDE), and decompose dense retrieval into two tasks, a generative task per- formed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder\\'s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks.  \\nHyDE appears unsupervised. No model is trained in HyDE: both the generative model and the con- trastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM.  \\nIn our experiments, we show HyDE using Instruct- GPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models significantly out- performs the previous state-of-the-art Contriever- only zero-shot no-relevance system on 11 queries  \\nsets, covering tasks like Web Search, Question Answering, Fact Verification and languages like Swahili, Korean, Japanese.'),\n",
      "  0.85015905)]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve relevant chunks based on the question\n",
    "\n",
    "docs = vector_store.similarity_search_with_relevance_scores(query=\"unsupervised denseretriever\", \n",
    "                                      k=3,\n",
    "                                      #search_type=\"similarity\",\n",
    "                                      score_threshold=0.8)\n",
    "\n",
    "#print(docs[0].page_content)\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
